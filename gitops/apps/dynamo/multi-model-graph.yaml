---
# Single-model DynamoGraphDeployment with gpt-oss-20b
# Pinned to sparky node (DGX Spark)
apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: multi-model
  namespace: dynamo
spec:
  # Use round-robin routing (simpler than KV-cache-aware routing)
  envs:
    - name: DYN_ROUTER_MODE
      value: "round-robin"
  services:
    # Frontend with round-robin routing
    Frontend:
      envFromSecret: hf-token-secret
      dynamoNamespace: multi-model
      componentType: frontend
      replicas: 1
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.0

    # GPT-OSS-20B - 20B MoE model
    GptOssWorker:
      envFromSecret: hf-token-secret
      dynamoNamespace: multi-model
      componentType: worker
      replicas: 1
      # Skip startup handshake to avoid frontend-worker discovery deadlock
      envs:
        - name: DYN_SKIP_STARTUP_HANDSHAKE
          value: "true"
      resources:
        limits:
          gpu: "1"
      extraPodSpec:
        runtimeClassName: nvidia
        nodeSelector:
          kubernetes.io/hostname: sparky
        volumes:
          - name: model-cache
            persistentVolumeClaim:
              claimName: model-storage
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.7.0
          workingDir: /workspace/examples/backends/vllm
          volumeMounts:
            - name: model-cache
              mountPath: /home/dynamo/.cache/huggingface
          command:
            - python3
            - -m
            - dynamo.vllm
          args:
            - --model
            - openai/gpt-oss-20b
            - --served-model-name
            - openai/gpt-oss-20b
            - --gpu-memory-utilization
            - "0.35"
            - --enforce-eager
