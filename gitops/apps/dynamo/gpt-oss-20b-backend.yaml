---
# vLLM Backend for gpt-oss-20b model
# This model is a 21B parameter MoE with 3.6B active params
# Can run in ~16GB VRAM with MXFP4 quantization
apiVersion: inference.nvidia.com/v1alpha1
kind: VLLMBackend
metadata:
  name: gpt-oss-20b
  namespace: dynamo
spec:
  # Model configuration
  model:
    name: openai/gpt-oss-20b
    source: huggingface

  # vLLM serving configuration
  vllm:
    # Use tensor parallelism for multi-GPU
    tensorParallelSize: 1
    # Enable quantization for better memory efficiency
    quantization: "fp8"
    # Max model length
    maxModelLen: 8192
    # GPU memory utilization
    gpuMemoryUtilization: 0.9

  # Resource requests
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "32Gi"
      cpu: "4"
    limits:
      nvidia.com/gpu: 1
      memory: "64Gi"
      cpu: "8"

  # Replicas
  replicas: 1

  # Node selector for GPU nodes
  nodeSelector:
    nvidia.com/gpu.present: "true"
